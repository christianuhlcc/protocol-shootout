<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Serialization Protocols</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/moon.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">


<!-- horrible indentation for a reason, because I don't want to read everything nested to far. -->

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        # Serialization Protocols in distributed Systems
        a shootout

        --

        I'm Chris

        My twitter is [@chrisuhl](https://twitter.com/chrisuhl)

        I work for a munich based startup called Matmatch

        <img class="plain" style="background:none; border:none; box-shadow:none;" src="media/matmatch.svg" height="150">

        We're a development team of 5 (and me) trying to change the world of materials


        ---

        ## Reasoning
        so why is this even important?

        --

        ## System designs are changing

        1. Microservice architectures are mainstream
        <!-- .element: class="fragment" -->
        1. Queues show up in mainstream architectures
        <!-- .element: class="fragment" -->
        1. CQRS / ES is a common design nowadays
        <!-- .element: class="fragment" -->
        1. We're not using databases as integration layer
        <!-- .element: class="fragment" -->

        --

        > Distributed systems are about protocols, not implementations. Forget languages, protocols are everything.

        [@TimPerret](https://twitter.com/timperrett/status/748590102926884864)


        --

        ### Evolvability

        > ... Evolutionary architecture, which supports incremental non-breaking change as a first principle along multiple dimensions ...

        [Rebecca Parsons, Neil Ford](https://www.thoughtworks.com/de/insights/blog/microservices-evolutionary-architecture)

        --

        ### Evolvability

        > The ability of a system and its data to adapt to change

        _Me, for the lack of a better definition_

        --

        ### Evolvability

        In monolithic systems with relational databases, a change in the data model was usually implemented by

        * a schema migration <!-- .element: class="fragment" -->
        * code changes <!-- .element: class="fragment" -->
        * one big deployment <!-- .element: class="fragment" -->

        --

		#### The Need For Evolvability

        * replicated systems <!-- .element: class="fragment" -->
        * distributed systems in our datacenter <!-- .element: class="fragment" -->
        * serverless, anyone? <!-- .element: class="fragment" -->
        * systems with code distributed to the user <!-- .element: class="fragment" -->

        --

        #### The Need For Evolvability

        All these systems need to exchange information pretty much all the time

        --

        ### Interrupt: why do we like to create new names so much?

        * to go out, data needs to be encoded/serialized/marshalled <!-- .element: class="fragment" -->

        * to get it back you need to decode/deserialize/unmarshall <!-- .element: class="fragment" -->

        --

        ### So how can we deal with this?

    </script>
</section>
<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Usual suspects

        --

        ### Programming Language provided serialization

        we get `java.Serializable` for free, so why not just use it?

        --

        No!
        <!-- .slide: style="color:white;font-size:5em" -->
        <!-- .slide: data-background-image="media/burning.jpg" -->
        <!-- .slide: data-background-position=center -->
        <!-- .slide: data-background-size=100%-->

        --

        ### NoooooooooOOoo

        * We are bound to JVM implementation languages (if you are really brave) <!-- .element: class="fragment" -->
        * The performance is awful <!-- .element: class="fragment" -->
        * It has some serious drawbacks <!-- .element: class="fragment" -->

        --

        > Implement `Serializable` judiciously

        [Joshua Bloch, Effective Java]

        * decreases the flexibility to change once released <!-- .element: class="fragment" -->
        * the implementation of the class becomes the API <!-- .element: class="fragment" -->
        * increases the likelihood of bugs and security holes <!-- .element: class="fragment" -->
        * increases the testing burden <!-- .element: class="fragment" -->

        --

        ### Serialization

        <img src="/media/mario_serialization.png">

        --

        #### Places where default serialization has already bitten me:

        * Akka remoting
        * Akka cluster
        * Spark
        * User Sessions with Spring Boot

        --

        ### Some notes about Performance

        |protocol| create |ser|deser|total|size|
        |--|--|--|--|--|--|
        |protobuf|121|1173|719|1891|239|
        |thrift|95|1455|731|2186|349|
        |kryo-flat|55|705|909|1614|268|
        |avro-generic|329|1721|984|2704|221|
        |scala/java-built-in|514|8280|36105|44385|1293|

        [Source: jvm-serializers wiki](https://github.com/eishay/jvm-serializers/wiki)

        --

        ### Some notes about performance

        * as long as its one of our four, it's probably fast enough
        * if you need it faster, you'll have to do your own research
        * just don't use the built in serializer

        --

        ### But... how about JSON? Or XML?

        + widespread
        + somewhat human readable
        - still slow (depending on the benchmark) <!-- .element: class="fragment" -->
        - verbose <!-- .element: class="fragment" -->
        - number handling is difficult <!-- .element: class="fragment" -->
        - character encoding is difficult <!-- .element: class="fragment" -->

        --

        ### But... how about JSON? Or XML?

        There also exists a various range of schema extensions for JSON, like BSON and JSON Schema.
        Unfortunately they don't provide schema evolvability, so I'll skip them


    </script>
</section>
<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Schema Evolvability

        --

        ### Reading Data between Schemas

        <img src="/media/serialization.png">


        --

        ### Backwards compatibility

        This means a newer schema can read data encoded with an older schema.

        That's halfway (not) easy. You know how the previous schema was, you can deal with missing things in your code. If you are aware of that.

        --

        ### Forwards compatibility

        Reading data from a schema that is newer that the schema of the running software.

        Ouch. How do you deal with things you don't know about?

        --

        ### Forwards compatibility

        Let a protocol handle this for us!

        We will compare 4 major Players:

        * Protobuf <!-- .element: class="fragment" -->
        * Thrift <!-- .element: class="fragment" -->
        * Avro <!-- .element: class="fragment" -->
        * Kryo <!-- .element: class="fragment" -->


    </script>
</section>
<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Protobuf

        --

        ### What is Protobuf (protocol buffers)?

        > Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler.

        [Protobuf Website](https://developers.google.com/protocol-buffers/)

        Besides the specification, Google provides a compiler for Java und uses it as a foundation for the Google Cloud Platform and gRPC

        --

        ### Protobuf Gradle Plugin

        Google provides a protocol buffer compiler `(protoc)` plugin for gradle. It will generate mighty Java Classes that can map and serialize Protobuf Classes

        ```
        classpath('com.google.protobuf:protobuf-gradle-plugin:0.8.7')
        ```

        --

        ### ScalaPB

        ScalaPB is a protocol buffer compiler `(protoc)` plugin for Scala. It will generate Scala case classes, parsers and serializers for your protocol buffers

        [Github](https://github.com/scalapb/ScalaPB)

        --

        ### Schema Sample

        ```
        syntax = "proto3";

        package de.christianuhl.proto;

        message Person {
            int32 user_id = 1;
            string firstname = 2;
            string lastname = 3;
        }
        ```

        --

        ### Scala Time!

        (live Sample)

        --

        ### Let's check the encoding on Disk

        |field tag| type |value |length |bytes / value |
        |--|--|--|--|--|--|
        |00010|010|12|09|43687269 73746961 6E|
        |00011|010|1A|03|0355686C|

        --

        ### Proto 2 vs Proto 3

        Significant changes:

        * No presence logic anymore
        * All values are optional by default, `required` semantic is removed
        * `Map` Type
        * Timevalues

        --

        ### Why the radical 'everything is optional'?

        * `required` breaks schema evolvability anyways
        * loads of production issues
        * was already banned in Google
        * `optional` became redundant and was removed as well

        --

        ### Java Time!

        (live Sample of an Event Store that accepts Protobuf)

    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Thrift

        --

        ### What is Thrift?

        > ... is an interface definition language and binary communication protocol that is used to define and create services for numerous languages. It is used as a remote procedure call (RPC) framework

        [Wikipedia](https://en.wikipedia.org/wiki/Apache_Thrift)

        --

        ### What is Thrift?

        * it has large conceptual similarities to protobuf
        * originally developed at Facebook
        * re-open-sourced later again as fbthrift

        --

        ## Java Integration

        * Apache Thrift Stuff exists, but is midldy complicated
        * Plugins who work pretty similar to what you've seen on protobuf

        [Sample](https://github.com/bsideup/spring-boot-thrift)

        --

        ## Scala & SBT Integration

        Twitter supplies scrooge

        [Scrooge](https://github.com/twitter/scrooge)

        --

        ## Schema Sample

        ```
        namespace java de.christianuhl.proto

        struct Person {
            1: optional i64     userId,
            2: required string  firstname,
            3: required string  lastname
        }
        ```

        --

        ### Scala Time!

        (live Sample)

        --

         ### Let's check the encoding on Disk

        |Type| Field Tag |Length |bytes | value |
        |--|--|--|--|--|
        |0B|0002|00000009|43687269 73746961 6E|Christian|
        |0B|0003|00000003|55686C|Uhl|

        --

        ### What makes Thrift special?

        * It's more than just serialization, it's a whole RPC framework

        * It embraces a whole family of encodings (They call it protocols)

        * Scala ecosystem only gets contributions by Twitter

        * otherwise, it's remarkably similar to protobuf

    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Avro

        --

        * started as a subproject of Hadoop, because Thrift was a bad fit for Hadoop

        * now an Apache project, supported by Confluent

        * Kafka (Confluent) is a large influencer nowadays

        * has two schema languages, one for humans and one for machines

        * direct mapping from and to JSON

        --

        differs from Protobuf and Thrift:

        * Reader and writer schema can be different, data brings its own schema

        * less type information for the payload needed

        * no manually assigned field ids


        ### All boils down to having different schemas for reading and writing

        --

        ## Schema Sample

        ```
        @namespace("de.christianuhl.avro")
        protocol avrosample {
        record Person {
            union {null, long} user_id = null;
            string firstname;
            string lastname;
        }}
        ```

        --

        ## Java Usage

        ```
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-avro-serializer</artifactId>
            <version>${confluent.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.avro</groupId>
            <artifactId>avro</artifactId>
            <version>${avro.version}</version>
        </dependency>
        ```

        [Sample](https://github.com/pjmeisch-cc/spring-boot-kafka-with-avro)

        --

        ### Usage with SBT

        using avrohugger and avro4s

        * [avro4s](https://github.com/sksamuel/avro4s)
        * [avrohugger](https://github.com/julianpeeters/avrohugger)

        --

        ### Scala Time!

        (live Sample)

        --

        ### Let's check the encoding on Disk


        first, 248 Bytes of schema

        |lenght| sing |value |bytes | value |
        |--|--|--|--|--|
        |0001001|0|12|43687269 73746961 6E|Christian|
        |0000011|0|06|55686C|Uhl|

        lastly 16 Byte i don't understand

        --

        ### So how do writer's and reader's schema go together?

        * they need to be _compatible_, but not equal
        * resolution rules are in the Avro specification
        * adding or removing only of fields with default values

        --

        ### How does Data and Schema find together?

        1. You include it in the payload (as in the sample)
        1. You versionize it, store it yourself and join it later (typical database application)
        1. Roll your schema registry and do Avro RPC

        --

        ### What benefits do we get from that?

        1. Support for dynamically generated schemas (Protobuf and Thrift want to be hand-crafted)
        1. Simpler interoperability with dynamically typed languages

        ### And the downsides

        1. Protocol overhead or runtime dependency, pick a poison
        1. Need to verify schema changes are compatible

    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Kryo

        --

        ### The 'odd' one of our little list

        * Object graph serialization framework for the JVM

        * No schema evolution

        * Blazing fast JVM-to-JVM communication

        * good candidate for live cluster communication (Akka e.g.)

        [Source](https://github.com/EsotericSoftware/kryo)

        --

        ### Java Sample


        ```
        ...
        import com.esotericsoftware.kryo.Kryo;
        import com.esotericsoftware.kryo.io.Output;
        // serializing
        Kryo kryo = new Kryo();
        AcmeMessage message = buildAcmeMessage(); // some domain object
        Output out = new Output(response.getOutputStream())
        kryo.writeObject(out, myObject);
        // deserializing
        Input in = new Input(request.getInputStream());
        AcmeMessage message = kryo.readObject(in, AcmeMessage.class);
        ```

        --

        ### Usage with SBT

        using twitter chill

        * [chill](https://github.com/twitter/chill)

        --

        ### Scala Time!

        (live Sample)

        --

        ### Let's check the encoding on Disk


        16 bytes, maximum density

        |field| type(?) |value (last byte starts with 1)| actual|
        |--|--|--|--|
        |01|01|4368 72697374 6961EE|Christian|
        |02|01|5568 EC|Uhl|

        --

        ### Kryo Summary

        * no schema

        * therefore no schema evolution

        * Good for message serialization within running instances

        * you do not want to persist these serialized values

    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Bonus Friend: Simple Binary Encoding

        --

        ### High Freqency optimized Serialization

        > SBE is an OSI layer 6 presentation for encoding and decoding binary application messages for low-latency financial applications. This repository contains the reference implementations in Java, C++, Golang, and C#.

        [Source](https://github.com/real-logic/simple-binary-encoding)

        --

        ### High Frequency optimized Serialization

        * crazily fast

        * from Real Logic

        * integrates well with the Aeron messaging system

        * supports schema evolution

        --

        ### Why is it not in this list?

        * Integration not so simple

        * ease of use vs performance

        * I cannot dissect the serialized bytes :)

    </script>
</section>

<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$">
    <script type="text/template">
        ## Summary

        --

        ## Summary

        |Framework  |Notes|
        |--|--|
        |Protobuf|😀 integration, 😀 concepts, 😫 2>3 version change|
        |Thrift|😀 concepts, 🤓 bigger framework, 😫 lacking integration|
        |Avro|😀 integration, 😀 for big files, 🤭 embedded schema concept|
        |Kryo|😍 speed, 😍 simplicity, 😢 no evolvability, 😢 not for persisting|

        --


        <!-- .slide: data-background-image="media/caution.jpg" -->
        <!-- .slide: style="color:white" -->
        Caution:

        `required` and schema evolution don't mix very well

        --

        ## Relevant Links

        * [Effective Java](https://www.amazon.de/Effective-Java-Joshua-Bloch/dp/0134685997)
        * [Desinging Data Intensive Applications](https://www.amazon.de/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321)
        (Everything I said in this talk is explained here, too)
        * [Schema evolution in Avro, Protocol Buffers and Thrift](http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html)

        --

        ## Questions?

        My twitter is [@chrisuhl](https://twitter.com/chrisuhl)

        Examples, slides and everything else:

        https://github.com/christianuhlcc/protocol-shootout

        I work for [Matmatch](matmatch.com) building a material search engine: We're cloud native and release 5 times a day to prod with a team of 5 - Ask me offline if you want to now more

        --

        ## Pictures:

        * [buring car](http://i.imgur.com/BerQeLQ.jpg)
        * [caution](https://flic.kr/p/9WfL6t)
    </script>
</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
                transition: 'none',
                slideNumber: 'c/t' ,
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
